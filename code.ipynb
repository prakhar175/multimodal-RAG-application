{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fc0d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prakh\\Main Codes\\Projects\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79ee062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "groq=os.getenv(\"GROQ_API_KEY\")\n",
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "llm=ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",api_key=groq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5f965",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_img(image_data):\n",
    "    \"\"\"Embedding the image using CLIP\"\"\"\n",
    "    if isinstance(image_data,str):\n",
    "        image=Image.open(image_data).convert('RGB')\n",
    "    else:\n",
    "        image=image_data\n",
    "    \n",
    "    input=clip_processor(images=image,return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_image_features(**input)\n",
    "        features = features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "def embed_text(text):\n",
    "    \"\"\"Embedding the image using CLIP\"\"\"\n",
    "    input=clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_text_features(**input)\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18a38c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path=\"input.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "all_docs=[]\n",
    "all_embeddings=[]\n",
    "image_data_store={}\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f574adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        temp_doc=Document(page_content=text,metadata={\"page\":i,\"type\":\"text\"})\n",
    "        text_chunks=splitter.split_documents([temp_doc])\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            embedding=embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "    \n",
    "    for img_index,img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref=img[0]\n",
    "            base_image=doc.extract_image(xref)\n",
    "            image_bytes=base_image[\"image\"]\n",
    "            #Conerting to PIL\n",
    "            pil_image=Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            #Create unique identifier\n",
    "            image_id=f\"page-{i}-img-{img_index}\"\n",
    "            #Store image as base64\n",
    "            buffered=io.BytesIO()\n",
    "            pil_image.save(buffered,format=\"PNG\")\n",
    "            img_base64=base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id]=img_base64\n",
    "            #Embed using CLIP\n",
    "            embedding=embed_img(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            #Doc for Image\n",
    "            image_doc=Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\":i,\"type\":\"image\",\"image_id\":image_id}\n",
    "            )    \n",
    "            all_docs.append(image_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image{img_index} on page {i}:{e}\")\n",
    "            continue\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b8ca556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "embeddins_array=np.array(all_embeddings)\n",
    "\n",
    "vector_store=FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content,emb) for doc,emb in zip(all_docs,embeddins_array)],\n",
    "    embedding=None,\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e82c5df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x253a79c8070>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbf60c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_modal(query,k=5):\n",
    "    \n",
    "    query_embed=embed_text(query)\n",
    "    \n",
    "    results=vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embed,\n",
    "        k=k\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6d39d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(query,retrieved_docs):\n",
    "    content=[]\n",
    "    content.append({\n",
    "        \"type\":\"text\",\n",
    "        \"text\":f\"Question: {query}\\n\\nContext:\"\n",
    "    })\n",
    "    \n",
    "    text_docs=[doc for doc in retrieved_docs if doc.metadata.get(\"type\")==\"text\"]\n",
    "    image_docs=[doc for doc in retrieved_docs if doc.metadata.get(\"type\")==\"image\"]\n",
    "    \n",
    "    if text_docs:\n",
    "        text_context=\"\\n\\n\".join([\n",
    "            f\"[Page{doc.metadata['page']}]: {doc.page_content}\"\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            \"type\":\"text\",\n",
    "            \"text\": f\"Text experts:\\n {text_context}\\n\"\n",
    "        })\n",
    "    for doc in image_docs:\n",
    "        image_id=doc.metadata.get(\"image_id\")\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                \"type\":\"text\",\n",
    "                \"text\":f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\":{\n",
    "                    \"url\":f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
    "                }\n",
    "            })\n",
    "            \n",
    "    content.append({\n",
    "        \"type\":\"text\",\n",
    "        \"text\":\"\\n\\n Please answer the question based on the context provided text and images\"\n",
    "    })\n",
    "    \n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a36991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_rag_pipeline(query):\n",
    "    context_docs=retrieve_modal(query,k=5)\n",
    "    \n",
    "    message =create_message(query,context_docs)\n",
    "    \n",
    "    response=llm.invoke([message])\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type=doc.metadata.get(\"type\",\"unknown\")\n",
    "        page=doc.metadata.get('page','?')\n",
    "        if doc_type ==\"text\":\n",
    "            preview=doc.page_content[:100]+\"...\" if len(doc.page_content)>100 else doc.page_content\n",
    "            print(f\" -Text from page{page}: {preview}\")\n",
    "        else:\n",
    "            print(f\" -imager from page{page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cdcd7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the aim of this guidance?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      " -Text from page35: 1990\n",
      "2000\n",
      "2010\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      " -Text from page25: men (low income countries)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      " -Text from page38: 15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "2006\n",
      "2011\n",
      "2016\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      " -Text from page13: to get insight into correlations and outliers.\n",
      " -Text from page4: Contents\n",
      "ACKNOWLEDGEMENTS..............................................................................\n",
      "\n",
      "\n",
      "Answer: Based on the context provided, I would say that the aim of this guidance is likely to:\n",
      "\n",
      "**Get insight into correlations and outliers.**\n",
      "\n",
      "This is directly stated on Page 13 of the context text. \n",
      "\n",
      "However, I also noticed that there is a specific section titled \"AIM OF THIS GUIDANCE\" on Page 4 (according to the Contents section), which likely provides a more detailed and explicit statement of the guidance's aim. Unfortunately, the text of that section is not provided in the context. \n",
      "\n",
      "If I had to make an educated guess, I would say that the aim of the guidance is probably related to analyzing and interpreting data, possibly in a field such as health or economics, given the presence of tables and graphs with data on pages 35, 25, and 38. \n",
      "\n",
      "If you have more information or text from Page VI, I may be able to provide a more accurate answer.\n",
      "======================================================================\n",
      "\n",
      "Query: what does the bubble chart means in Fig. 10\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      " -Text from page26: of a slice when it is close to 0%, 25%, 50%, 75% or 100%. Any percentages other than these are \n",
      "diff...\n",
      " -Text from page5: 5.6 Avoiding pie charts................................................................................\n",
      " -Text from page0: Tools for making good \n",
      "data visualizations:\n",
      "\n",
      "the art of charting\n",
      " -Text from page2: Tools for making \n",
      "good data \n",
      "visualizations:\n",
      "\n",
      "the art of charting\n",
      " -Text from page13: to get insight into correlations and outliers.\n",
      "\n",
      "\n",
      "Answer: There is no Fig.10 or any image provided in the context. The text appears to be discussing best practices for data visualization, specifically the use of pie charts versus bar charts.\n",
      "\n",
      "However, based on the text, it seems that the author is arguing against the use of pie charts and in favor of bar charts, citing the difficulty of accurately discerning percentages in a pie chart.\n",
      "\n",
      "If you could provide the actual image of Fig.10, I would be happy to help you interpret the bubble chart.\n",
      "======================================================================\n",
      "\n",
      "Query: What dooes the table Anscombe’s quartet means\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      " -Text from page12: 1. Why data visualization is needed\n",
      "4\n",
      "Table 1. Anscombe’s quartet\n",
      "Dataset I\n",
      "Dataset II\n",
      "Dataset III\n",
      "D...\n",
      " -Text from page25: TSJ\n",
      "TSJ\n",
      "MAL\n",
      "MAL\n",
      "SLK\n",
      "SLK\n",
      "EST\n",
      "EST\n",
      "KRO\n",
      "KRO\n",
      "HON\n",
      "HON\n",
      "POL\n",
      "POL\n",
      "LIT\n",
      "LIT\n",
      "LET\n",
      "LET\n",
      "ROE\n",
      "ROE\n",
      "BULG\n",
      "BULG\n",
      "RUS\n",
      "RUS\n",
      "KA...\n",
      " -Text from page35: 1990\n",
      "2000\n",
      "2010\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      " -Text from page13: to get insight into correlations and outliers.\n",
      " -Text from page25: men (low income countries)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "\n",
      "\n",
      "Answer: Based on the context provided, Anscombe's Quartet refers to a set of four datasets (I, II, III, and IV) presented in a table (Table 1) on page 12 of the text. \n",
      "\n",
      "Anscombe's Quartet is a famous example in statistics that illustrates the importance of data visualization. The four datasets have similar summary statistics (e.g., mean, variance, correlation), but when visualized, they exhibit distinct patterns and relationships between the variables.\n",
      "\n",
      "The table shows the datasets with two variables, x and y, for each of the four datasets. A brief examination of the table reveals:\n",
      "\n",
      "* Dataset I: A somewhat linear relationship between x1 and y1.\n",
      "* Dataset II: A perfect parabolic relationship between x2 and y2.\n",
      "* Dataset III: A perfect linear relationship between x3 and y3, but with one outlier.\n",
      "* Dataset IV: A single point that determines the relationship between x4 and y4.\n",
      "\n",
      "The purpose of presenting these datasets is \"to get insight into correlations and outliers\" (page 13), highlighting the limitations of relying solely on summary statistics to understand the relationships between variables. Data visualization is essential to uncover the underlying patterns and structures in the data.\n",
      "\n",
      "In essence, Anscombe's Quartet demonstrates that:\n",
      "\n",
      "1. Summary statistics can be similar for different datasets.\n",
      "2. Data visualization can reveal distinct patterns and relationships between variables.\n",
      "3. Correlation and regression analysis should be accompanied by data visualization to ensure a deeper understanding of the data.\n",
      "\n",
      "By examining the table and understanding the implications of Anscombe's Quartet, one can appreciate the importance of data visualization in statistical analysis.\n",
      "======================================================================\n",
      "\n",
      "Query: what does the bubble chart means in Fig. 10\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 5 documents:\n",
      " -Text from page26: of a slice when it is close to 0%, 25%, 50%, 75% or 100%. Any percentages other than these are \n",
      "diff...\n",
      " -Text from page5: 5.6 Avoiding pie charts................................................................................\n",
      " -Text from page0: Tools for making good \n",
      "data visualizations:\n",
      "\n",
      "the art of charting\n",
      " -Text from page2: Tools for making \n",
      "good data \n",
      "visualizations:\n",
      "\n",
      "the art of charting\n",
      " -Text from page13: to get insight into correlations and outliers.\n",
      "\n",
      "\n",
      "Answer: Unfortunately, I don't see Fig.10 or any image in the context you provided. However, I can still try to help you understand what a bubble chart typically means.\n",
      "\n",
      "A bubble chart is a type of data visualization that displays data as a collection of circles (bubbles) on a two-dimensional grid. The size of each bubble represents a third dimension of data, in addition to the x-axis and y-axis.\n",
      "\n",
      "Typically, a bubble chart is used to:\n",
      "\n",
      "1. Show relationships between three variables.\n",
      "2. Identify patterns, correlations, or outliers in the data.\n",
      "3. Visualize clusters or groupings in the data.\n",
      "\n",
      "Without seeing Fig.10, I can only speculate that the bubble chart in Fig.10 might be used to:\n",
      "\n",
      "* Display correlations between different variables (as mentioned on [Page13] of your context).\n",
      "* Show how different categories or groups relate to each other in terms of multiple variables.\n",
      "\n",
      "If you can provide more context or describe Fig.10, I'd be happy to try and give a more specific answer.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "        \"What is the aim of this guidance?\",\n",
    "        \"what does the bubble chart means in Fig. 10\",\n",
    "        \"What dooes the table Anscombe’s quartet means\",\n",
    "        \"what does the bubble chart means in Fig. 10\"\n",
    "    ]\n",
    "    \n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    answer = multimodal_rag_pipeline(query)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b1967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
